openModeller Acceptance Test Script
===================================

This directory contains a python script designed to perform extensive tests
on many of openModeller features to ensure its quality.

The tests should be run every time that significant changes have been made
to the code base as well as when new releases are being prepared.


INSTRUCTIONS
------------

Here are some instructions on how to setup and run the tests:

1) Configure openModeller to build the Python interface. This can be done by
running the following commands on OM root directory:

	./autogen.sh
	./configure --enable-python
	make
	make install

2) Set your PYTHON_PATH environmental variable to include Python site-packages
directory. In Linux Fedora 2 and Python 2.3, the default directory is:

	/usr/lib/python2.3/site-packages

3) Run the tests with the command:

	./runtest.sh

First, the test script will download test data from 
http://www.cria.org.br/~ricardo/openModeller/acc_test_data.tar.gz.
Then it will untar it locally and run the full python test script on it.

4) The test will run and results will be output to html reports in the file:

	SummaryReports.html

That page shows a summary of test results. There is a link to a second 
html page that shows the actual maps generated during the tests side by 
side to sample of expected results.

One would browse through the maps visually comparing actual results
agains expected samples.


COMMAND LINE OPTIONS
--------------------

The test script can be configured to run only a subset of the tests. To 
do that, use the following command line switches:

-g, --group:     Specify the test group to be run
                 1: Create models of 7 species with all OM available algorithms
                 2: Runs one GARP model and generates maps using several 
                   different output formats

-s, --species:   Run modes on group 1 for only the specified species 
                 index (0-6).

-a, --algorithm: Specify which algorithm to run, defined by its ID

-f, --full:      Force execution of all specified tests (even if test 
                 results are already present in the directory).


CURRENT STATUS
--------------

- Need to add expected results images;
 
- Most failures during test make the script abort thus not generating the
final result files. It would be nice if tests were more robust to keep going
even if a particular test fails;

- Need to add those tests to autotools setup. That would include a separate
make action, that in turn would include automatic download and extraction 
of test datasets. The test would require python interface to be enabled. 



ACKNOWLEDGEMENTS
----------------

Many thanks to Peter Brewer and A. Townsend Peterson for contributing with
some of the datasets used in this test script.

------------ x -----------


